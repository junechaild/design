function zero_isnot_minimizer = gradient_validation(MainData, W, NewBatch, ModelPara)
% zero_isnot_minimizer = gradient_validation(MainData, W, NewBatch)
% ||X*W*W'-X||_F^2 + alpha*||W||_2,1
% This the speed of this function depends on the sample size, but not the dimension
    [t, k] = size(W);
    [n, t] = size(MainData);
    eps = 0.1;
    alpha = ModelPara.alpha;
    N = 100;% 0附近的N个随机点
    point_set = eps*rand(k, N) - 0.5*eps; % 立方体随机采样法，eps为立方体边长
    M = [MainData, NewBatch]'*[MainData, NewBatch];
    D_vector = 0.5./sqrt(sum(W.*W, 2));
% 利用矩阵分裂手段，分离出计算梯度时的重复部分：
% subgradient = 2*(alpha*D - X'X)W
%             = 2*alpha(D_old*W_old + D_old*W_incre + D_incre*W_old +
%             D_incre* W_incre) - 2*M*W_old - 2*M*W_incre
%             =  (2*alpha*D_old*W_old - 2*M*W_old) +  (提前计算)
%             (2*alpha*D_incre*W_incre - 2*M*W_incre) (每次在新的测试点计算)
    W_old = [W;zeros(1, k)];
    D_old = diag([D_vector;0]);
    gradient_old = 2*alpha*D_old*W_old - 2*M*W_old;
    zero_isnot_minimizer = 0;
    
    for i=1:N %依次测试N个点的梯度是否满足一阶Taylor增量条件
        W_come = point_set(:, i)';
        % W_incre = zeros(t+1, k); W_incre(t+1,:) = W_come
        % 可验证，M*W_incre等价于M(:,t+1)*W_come，进一步把3次计算复杂度降为2次
        gradient_incre = [zeros(t, k);alpha*W_come/norm(W_come, 2)] - 2*M(:,t+1)*W_come;
        subgradient = gradient_old + gradient_incre;
        
        % f在非0处可导  ===>  f(0) = f(W_come) + gradient_f(W_come)*(-W_come) + O(...)
        % f(0)局部最小 <===>  gradient_f(W_come)*(-W_come) <= 0
        if subgradient*point_set(:, i) < 0
            zero_isnot_minimizer = 1;
            break;
        end
    end
    if i<=N
        fprintf('pass the gradient validation in the point %d\n',i);
    else
        fprintf('did not pass the gradient validation\n');
    end
end